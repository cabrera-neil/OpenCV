{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OpenCV-Python Tutorial] Classification\n",
    "\n",
    "In this notebook, we will learn how to perform Object Classification using Visual Bag of Words(VBoW), spatial histogram feature, and SVM classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e717cb6dae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Pickle'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os, sys\n",
    "import tarfile\n",
    "import time\n",
    "import random\n",
    "import Pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech_url = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'\n",
    "caltech_filename = '101_ObjectCategories.tar.gz'\n",
    "caltech_dir = '101_ObjectCategories/'\n",
    "\n",
    "numTrain = 15\n",
    "numTest = 15\n",
    "numClasses = 102\n",
    "numWords = 600\n",
    "\n",
    "vocabPath = os.path.join(caltech_dir, 'vocab.pkl')\n",
    "svmPath = os.path.join(caltech_dir, 'svm_data.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Caltech-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, path):\n",
    "    import urllib2\n",
    "    file_name = url.split('/')[-1]\n",
    "    u = urllib2.urlopen(url)\n",
    "    f = open(os.path.join(path, file_name), 'wb')\n",
    "    meta = u.info()\n",
    "    file_size = int(meta.getheaders(\"Content-Length\")[0])\n",
    "    print \"Downloading: %s Bytes: %s\" % (file_name, file_size)\n",
    "\n",
    "    download_size = 0\n",
    "    block_size = 8192\n",
    "    while True:\n",
    "        buf = u.read(block_size)\n",
    "        if not buf:\n",
    "            break\n",
    "        download_size += len(buf)\n",
    "        f.write(buf)\n",
    "        status = \"\\r%12d  [%3.2f%%]\" % (download_size, download_size * 100. / file_size)\n",
    "        print status,\n",
    "        sys.stdout.flush()\n",
    "    f.close()\n",
    "\n",
    "if not os.path.exists(caltech_dir) or not os.path.exists(os.path.join(caltech_dir, 'airplanes')):\n",
    "    print 'Downloading Caltech-101'\n",
    "    if not os.path.exists(caltech_filename) or os.path.getsize(caltech_filename) != 131740031:\n",
    "        download_file(caltech_url, '.')\n",
    "    print 'Extracting Caltech-101'\n",
    "    with tarfile.open(caltech_filename) as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "print 'Caltech-101 dataset checked'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 15 train/test images for each class\n",
    "print 'Select 15 train/test images for each class'\n",
    "classes = os.walk(caltech_dir).next()[1]\n",
    "train_image_ll = []\n",
    "test_image_ll = []\n",
    "for c in classes:\n",
    "    class_dir = os.path.join(caltech_dir, c)\n",
    "    ims = [f for f in os.walk(class_dir).next()[2] if f.endswith('.jpg')]\n",
    "    random.shuffle(ims)\n",
    "    train_image_ll.append([os.path.join(caltech_dir,c,f) for f in ims[:numTrain]])\n",
    "    test_image_ll.append([os.path.join(caltech_dir,c,f) for f in ims[numTrain:numTrain+numTest]])\n",
    "print 'done'\n",
    "\n",
    "# Show 4 random images\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, idx in enumerate(random.sample(range(len(classes)), 4)):\n",
    "    fpath = random.sample(train_image_ll[idx], 1)[0]\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(fpath), cv2.COLOR_BGR2RGB))\n",
    "    plt.xlabel(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense SIFT(PHOW) and Visual Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SIFT helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Resize a image if it's too large\n",
    "def standarizeImage(img):\n",
    "    rows, cols = img.shape\n",
    "    if cols > 480:\n",
    "        img = cv2.resize(img, (480, rows*480/cols))\n",
    "    return img\n",
    "\n",
    "# Detect and extract SIFT from a single image\n",
    "def SIFT(img):\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return des\n",
    "\n",
    "# Load an image + Resize if large + Extract SIFT\n",
    "def SIFT2(img_fpath):\n",
    "    img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    img = standarizeImage(img)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    return SIFT(img)\n",
    "\n",
    "# Dense SIFT(Extract SIFT descriptor in grid points over an image)\n",
    "def denseSIFT(img, step = 5, size = 7):\n",
    "    rows, cols = img.shape[:2]\n",
    "    kp = []\n",
    "    for x in xrange(step,cols,step):\n",
    "        for y in xrange(step,rows,step):\n",
    "            kp.append(cv2.KeyPoint(x, y, size))\n",
    "    kp, des = sift.compute(img, kp)\n",
    "    return des\n",
    "\n",
    "# Load an image + Resize if large + Dense SIFT\n",
    "def denseSIFT2(img_fpath, step = 10):\n",
    "    img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    img = standarizeImage(img)\n",
    "    return denseSIFT(img, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Words(Train Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vocabulary\n",
    "if not os.path.exists(vocabPath):\n",
    "    # Get PHOW features from 30 random training image to build a dictionary\n",
    "    print \"Extracting PHOW features some training images...\"\n",
    "    PHOW_descrs = []\n",
    "    temp = [item for sublist in train_image_ll for item in sublist]\n",
    "    temp = random.sample(temp, 30)\n",
    "    for fpath in temp:\n",
    "        des = denseSIFT2(fpath)\n",
    "        PHOW_descrs.append(des)\n",
    "    PHOW_descrs = np.concatenate(PHOW_descrs, axis=0)\n",
    "    print \"Total %d PHOW features\" % PHOW_descrs.shape[0]\n",
    "    \n",
    "    # Quantize the descriptors to get the visual words\n",
    "    print \"Running K-means clustering (%d -> %d)...\" % (PHOW_descrs.shape[0], numWords)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 500, 1.0)\n",
    "    attempts = 10\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    start_time = time.time()\n",
    "    retval, bestLabels, vocab = cv2.kmeans(PHOW_descrs, numWords, None, criteria, attempts, flags)\n",
    "    print('Elapsed time: %.6fs' % (time.time() - start_time))\n",
    "    \n",
    "    print 'Saving...' \n",
    "    with open(vocabPath, 'wb') as fd:\n",
    "        pickle.dump(vocab, fd)\n",
    "else:\n",
    "    print \"Load the trained visual words...\"\n",
    "    with open(vocabPath) as fd:\n",
    "        vocab = pickle.load(fd)\n",
    "\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "myCounter = collections.Counter(bestLabels.flatten())\n",
    "plt.plot(sorted(myCounter.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "\n",
    "def getImageDescriptor(img, step=5, size=7):\n",
    "    img = standarizeImage(img)\n",
    "    cols, rows = img.shape[:2]\n",
    "    \n",
    "    # Extracting denseSIFT and BoW\n",
    "    kp = []\n",
    "    for x in xrange(step,cols,step):\n",
    "        for y in xrange(step,rows,step):\n",
    "            kp.append(cv2.KeyPoint(x, y, size))\n",
    "    kp, des = sift.compute(img, kp)\n",
    "    matches = bf.knnMatch(des, vocab, k=1)\n",
    "    words = [m[0].trainIdx for m in matches]\n",
    "    \n",
    "    # Spatial Binning - 2x2\n",
    "    binX = 2; binY = 2;\n",
    "    temp = np.zeros((binX, binY, numWords), dtype=np.float32)\n",
    "    for k, w in zip(kp, words):\n",
    "        i = (int)(k.pt[0]) * binX / cols\n",
    "        j = (int)(k.pt[1]) * binY / rows\n",
    "        temp[i, j, w] += 1\n",
    "    for i in range(binX):\n",
    "        for j in range(binY):\n",
    "            temp[i, j, :] /= np.sum(temp[i, j, :])\n",
    "    temp = temp.flatten()\n",
    "    hist = temp\n",
    "    \n",
    "    # Spatial Binning - 4x4\n",
    "    binX = 4; binY = 4;\n",
    "    temp = np.zeros((binX, binY, numWords), dtype=np.float32)\n",
    "    for k, w in zip(kp, words):\n",
    "        i = (int)(k.pt[0]) * binX / cols\n",
    "        j = (int)(k.pt[1]) * binY / rows\n",
    "        temp[i, j, w] += 1\n",
    "    for i in range(binX):\n",
    "        for j in range(binY):\n",
    "            temp[i, j, :] /= np.sum(temp[i, j, :])\n",
    "    temp = temp.flatten()\n",
    "    hist = np.concatenate((hist, temp), axis=0)\n",
    "    \n",
    "    hist /= np.sum(hist)\n",
    "    return hist    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Genarate spatial histogram for Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spartial histogram for all training images\n",
    "print 'Extract spartial histogram for all training images'\n",
    "BoW_train_ll = []\n",
    "for c, image_list in zip(classes, train_image_ll):\n",
    "    print c, \n",
    "    temp = []\n",
    "    for fpath in image_list:\n",
    "        img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "        temp.append(getImageDescriptor(img))\n",
    "    BoW_train_ll.append(temp)\n",
    "\n",
    "print '\\ndone!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training data(BoW, label) for SVM\n",
    "print 'Setting training data(BoW, label) for SVM'\n",
    "train_bow = []\n",
    "train_labels = []\n",
    "numSpartialHist = len(BoW_train_ll[0][0])\n",
    "for i, BoW_list in enumerate(BoW_train_ll):\n",
    "    for bow in BoW_list:\n",
    "        # Hellinger's kernel for each training instance(bow)\n",
    "        temp = np.sqrt(bow)\n",
    "        temp -= np.average(temp)\n",
    "        if np.std(temp) != 0:\n",
    "            temp /= np.std(temp)\n",
    "        train_bow.append([temp])\n",
    "    train_labels.extend([i] * len(BoW_list))\n",
    "train_bow = np.concatenate(train_bow, axis=0)\n",
    "train_labels = np.array(train_labels, dtype=np.int).reshape((numTrain*numClasses,1))\n",
    "# => train_bow: (1530, 12000), train_labels: (1530,1)\n",
    "\n",
    "print \"train_bow: \" + str(train_bow.shape)\n",
    "print \"train_labels: \" + str(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "print 'Training SVM...'\n",
    "svm = cv2.ml.SVM_create()\n",
    "svm.setType(cv2.ml.SVM_C_SVC)\n",
    "svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "svm.setC(0.01)\n",
    "svm.setTermCriteria((cv2.TERM_CRITERIA_COUNT, 10, 1.0))\n",
    "\n",
    "start_time = time.time()\n",
    "# svm.train(SVM_train_data)\n",
    "svm.train(train_bow, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "print('Elapsed time: %.6fs' % (time.time() - start_time))\n",
    "\n",
    "# print 'Saving SVM...'\n",
    "# svm.save(svmPath)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = svm.predict(train_bow)[1]\n",
    "print('Training Accuracy: %.6f' % np.average(train_preds == train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test trained SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spartial histogram for all test images\n",
    "print 'Extract spartial histogram for all test images\\n'\n",
    "BoW_test_ll = []\n",
    "for c, image_list in zip(classes, test_image_ll):\n",
    "    print c, \n",
    "    temp = []\n",
    "    for fpath in image_list:\n",
    "        img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "        temp.append(getImageDescriptor(img))\n",
    "    BoW_test_ll.append(temp)\n",
    "\n",
    "print '\\ndone!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting test data(BoW, label) for SVM\n",
    "print 'Setting test data(BoW, label) for SVM'\n",
    "test_bow = []\n",
    "test_labels = []\n",
    "numSpartialHist = len(BoW_test_ll[0][0])\n",
    "for i, BoW_list in enumerate(BoW_test_ll):\n",
    "    for bow in BoW_list:\n",
    "        temp = np.sqrt(bow)\n",
    "        temp -= np.average(temp)\n",
    "        if np.std(temp) != 0:\n",
    "            temp /= np.std(temp)\n",
    "        test_bow.append([temp])\n",
    "    test_labels.extend([i] * len(BoW_list))\n",
    "test_bow = np.concatenate(test_bow, axis=0)\n",
    "test_labels = np.array(test_labels, dtype=np.int).reshape((numTest*numClasses,1))\n",
    "# => test_bow: (1530, 200), test_labels: (1530,1)\n",
    "\n",
    "print test_bow.shape\n",
    "print test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = svm.predict(test_bow)[1]\n",
    "print('Test Accuracy: %.6f' % np.average(test_preds == test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 8 random images\n",
    "plt.figure(figsize=(16,10))\n",
    "for i, idx in enumerate(random.sample(range(len(classes)), 8)):\n",
    "    # Extract Image Descriptor\n",
    "    fpath = random.sample(test_image_ll[idx], 1)[0]\n",
    "    img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n",
    "    bow = getImageDescriptor(img)\n",
    "    \n",
    "    # Hellinger's kernel\n",
    "    temp = np.sqrt(bow)\n",
    "    temp -= np.average(temp)\n",
    "    if np.std(temp) != 0:\n",
    "        temp /= np.std(temp)\n",
    "    \n",
    "    # Prediction\n",
    "    test_preds = (int)(svm.predict(temp.reshape((1,12000)))[1][0,0])\n",
    "    \n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(fpath), cv2.COLOR_BGR2RGB))\n",
    "    plt.xlabel(\"GT: %s / Predict: %s\" % (classes[idx], classes[test_preds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Practice: Classification for your own image\n",
    "\n",
    "Choose Classify your own test image using our classification.\n",
    "\n",
    "- Place your image in the path where this notebook can access\n",
    "- Load an image as grayscale\n",
    "- Extract Dense SIFT BoW with `getImageDescriptor()`\n",
    "- Normalize the bow with Hellinger's kernel\n",
    "- Predict using the SVM classifier we have trained so far(`svm` instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = 'images/dragonfly.jpg'  # Path to your own image\n",
    "img = cv2.imread(img_fpath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#================ YOUR CODE HERE ===================\n",
    "\n",
    "\n",
    "prediction = 0  # Predicted class of your image\n",
    "#===================================================\n",
    "\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(img_fpath), cv2.COLOR_BGR2RGB))\n",
    "plt.xlabel(\"Prediction: %s\" % classes[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
